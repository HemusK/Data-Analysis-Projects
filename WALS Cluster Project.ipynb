{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to import our dataset as a dataframe in order to cluster it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALS_raw = pd.read_csv('WALS table.csv')\n",
    "WALS_languages = pd.read_csv('WALS languages.csv')\n",
    "WALS_data = WALS_raw.drop(columns=\"label\")\n",
    "WALS_languages.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has features with measurements of different lengths. It is entirely categorical so the numbers are fairly similar, nontheless we should normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "WALS_normalized = pd.DataFrame(scaler.fit_transform(WALS_data), index=WALS_raw.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this dataset has a hundred-some features, in order to cluster it and visualize it more effectively, we are going to use dimensionality reduction to get it to two features. There are a number of different methods to reduce dimensionality, but I will be using t-Distributed Stochastic Neighbor Embedding. This will better preserve the local distances than another algorithm like Principal Component Analaysis, so it will be better for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSNEReduction = TSNE(n_components=2)\n",
    "WALS_reduced = pd.DataFrame(TSNEReduction.fit_transform(WALS_normalized), index=WALS_raw.index)\n",
    "WALS_reduced.columns = [\"x\", \"y\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's plot it to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(WALS_reduced, x=\"x\", y=\"y\", hover_name=WALS_raw[\"label\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this, the t-SNE reduction has given us a lot of areas with a lot of density. However, it's hard to count the exact number of clusters as there are many areas that could be clustered in many different ways due to small changes in density. From looking at this, it looks to me that the most optimal solution would be a density-based clustering algorithm, as it will automatically select the number of clusters based on areas of high density. The one disadvantage of this type of clustering is that it assigns some points as outliers, but that works for this project because we can think of those as \"language isolates\", languages that do not have any living relatives. The specific clustering algorithm I am going to use is HDBSCAN, an algorithm that converts DBSCAN into a hierarchical clustering algorithm. What this means is that it not only is looking for areas of high density, but it is creating a hierarchical categorization of the distances between the points. This allows it to account for clusters of varying density, something which DBSCAN has trouble with since it assumes anything outside of its predetermined density radius (epsilon) is in a different cluster or an outlier. Essentially, HDBSCAN operates like a DBSCAN with a variable epsilon based on the cluster.\n",
    "\n",
    "So now I am going to apply HDBSCAN to our dataset, and plot the results. Additionally, I merged the now clustered data points with the WALS languages.csv I imported earlier. This csv contains more identifying information about the languages, such as where they are located and what language family they are in, and most importantly their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster = HDBSCAN(min_cluster_size=10, min_samples=10, cluster_selection_epsilon=2.7)\n",
    "WALS_prediction = pd.DataFrame(Cluster.fit_predict(WALS_reduced), index=WALS_reduced.index)\n",
    "WALS_prediction.columns = [\"cluster\"]\n",
    "WALS_clustered = pd.concat([WALS_reduced, WALS_prediction], axis=1)\n",
    "WALS_clustered[\"ID\"] = WALS_raw[\"label\"]\n",
    "WALS_clustered = WALS_clustered.merge(WALS_languages, how=\"inner\", on=\"ID\")\n",
    "WALS_clustered[\"cluster\"] = WALS_clustered[\"cluster\"].astype(str)\n",
    "px.scatter(WALS_clustered, x=\"x\", y=\"y\", color=\"cluster\", hover_name=\"Name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browsing around this, you can see that some similar languages are clustered. One significant one that stands out is the small cluster in the bottom left corner around (-45,45). This cluster entirely consists of sign languages, a fairly natural grouping. But just by a cursory look it doesn't seem to correspond much with language families otherwise. Let's see how the plot looks when colored based on language family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(WALS_clustered, x=\"x\", y=\"y\", color=\"Family\", hover_name=\"Name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the clusters don't correspond well to language families. To see if it still might correspond to areal relationships, let's see how this looks on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_geo(WALS_clustered, lat=\"Latitude\", lon=\"Longitude\", color=\"cluster\", hover_name=\"Name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this does not correspond well with geographic area either. This ultimately illustrates the point that we began with, that typological similarities are not sufficient basis for languages to be treated as part of the same family."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
